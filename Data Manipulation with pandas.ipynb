{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb0671-4747-4282-bd70-2da5f54dca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting a DataFrame\n",
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print a description of homelessness\n",
    "print(homelessness.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf1e67-62ac-4607-9cd0-6ba7aa1ad087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parts of a DataFrame\n",
    "# Import pandas using the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Print the values of homelessness\n",
    "print(homelessness.values)\n",
    "\n",
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)\n",
    "\n",
    "# Print the row index of homelessness\n",
    "print(homelessness.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f87a01-07dc-4ccc-9f01-1428985df338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting and subsetting\n",
    "#Sorting rows\n",
    "# Sort homelessness by individual\n",
    "homelessness_ind = homelessness.sort_values(\"individuals\")\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_ind.head())\n",
    "\n",
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = homelessness.sort_values(\"family_members\", ascending = False)\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_fam.head())\n",
    "\n",
    "# Sort homelessness by region, then descending family members\n",
    "homelessness_reg_fam = homelessness.sort_values([\"region\", \"family_members\"], ascending=[True, False])\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_reg_fam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9287ead-f300-4c68-9749-924b9c549a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting columns\n",
    "# Select the individuals column\n",
    "individuals = homelessness[\"individuals\"]\n",
    "\n",
    "# Print the head of the result\n",
    "print(individuals.head())\n",
    "\n",
    "# Select the state and family_members columns\n",
    "state_fam = homelessness[[\"state\", \"family_members\"]]\n",
    "\n",
    "# Print the head of the result\n",
    "print(state_fam.head())\n",
    "\n",
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = homelessness[[\"individuals\", \"state\"]]\n",
    "\n",
    "# Print the head of the result\n",
    "print(ind_state.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f2f57-89c7-4c51-a666-9494482930e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting rows\n",
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = homelessness[homelessness[\"individuals\"] > 10000]\n",
    "\n",
    "# See the result\n",
    "print(ind_gt_10k)\n",
    "\n",
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = homelessness[homelessness[\"region\"] == \"Mountain\"]\n",
    "\n",
    "# See the result\n",
    "print(mountain_reg)\n",
    "\n",
    "# Filter for rows where family_members is less than 1000 \n",
    "# and region is Pacific\n",
    "fam_lt_1k_pac = homelessness[(homelessness[\"family_members\"] < 1000) & (homelessness[\"region\"] == \"Pacific\")]\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700f944-15f3-4fac-ab80-429f02212a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting rows by categorical variables\n",
    "# Subset for rows in South Atlantic or Mid-Atlantic regions\n",
    "south_mid_atlantic = homelessness[(homelessness[\"region\"] == \"South Atlantic\") | (homelessness[\"region\"] == \"Mid-Atlantic\")]\n",
    "\n",
    "# See the result\n",
    "print(south_mid_atlantic)\n",
    "\n",
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_homelessness = homelessness[homelessness[\"state\"].isin(canu)]\n",
    "\n",
    "# See the result\n",
    "print(mojave_homelessness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ed7a8-95b3-4ba0-8035-61bf6b90155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New columns\n",
    "#Adding new columns\n",
    "# Add total col as sum of individuals and family_members\n",
    "homelessness[\"total\"] = homelessness[\"individuals\"] + homelessness[\"family_members\"]\n",
    "\n",
    "# Add p_individuals col as proportion of individuals\n",
    "homelessness[\"p_individuals\"] = homelessness[\"individuals\"] / homelessness[\"total\"]\n",
    "\n",
    "# See the result\n",
    "print(homelessness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe49db6-897c-4551-a51a-36ff33972390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combo-attack!\n",
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"] \n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending = False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = high_homelessness_srt[[\"state\", \"indiv_per_10k\"]]\n",
    "\n",
    "# See the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacb187-4fae-4cfa-8687-ee6a5cc85666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics\n",
    "#Mean and median\n",
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales[\"weekly_sales\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21fde6e-02cd-4d2c-9ffc-8ecb4fc75ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing dates\n",
    "# Print the maximum of the date column\n",
    "print(sales[\"date\"].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales[\"date\"].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3dc637-d732-4db2-893f-0ed27c531727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Efficient summaries\n",
    "## A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# Print IQR of the temperature_c column\n",
    "print(sales[\"temperature_c\"].agg(iqr))\n",
    "\n",
    "\n",
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))\n",
    "\n",
    "# Import NumPy and create custom IQR function\n",
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2bd156-c1b2-4999-876c-dc9e8c0b55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative statistics\n",
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales_1_1.sort_values(\"date\", ascending = True)\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
    "sales_1_1[\"cum_weekly_sales\"] = sales_1_1[\"weekly_sales\"].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
    "sales_1_1[\"cum_max_sales\"] = sales_1_1[\"weekly_sales\"].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b556e74-866a-43fa-bc03-2a3c388a395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting\n",
    "#Dropping duplicates\n",
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates([\"store\", \"type\"])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates([\"store\", \"department\"])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales[\"is_holiday\"]].drop_duplicates(\"date\")\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f5916-f098-4781-b3c3-80ad89a7e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting categorical variables\n",
    "# Count the number of stores of each type\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types[\"type\"].value_counts(normalize = True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of each department number and sort\n",
    "dept_counts_sorted = store_depts[\"department\"].value_counts()\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of departments of each number and sort\n",
    "dept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f8bd5-feef-4cd3-91d4-bc0f6acddb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped summary statistics\n",
    "#What percent of sales occurred at each store type?\n",
    "# Calc total weekly sales\n",
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1a703-919f-4d7a-847a-6b43ced82e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculations with .groupby()\n",
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "print(sales)\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum() / sum(sales_by_type)\n",
    "print(sales_propn_by_type)\n",
    "\n",
    "# From previous step\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby([\"type\", \"is_holiday\"])[\"weekly_sales\"].sum()\n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da916e71-dc42-4fda-a8e8-0cb1cea78736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple grouped summaries\n",
    "# Import numpy with the alias np\n",
    "import numpy as np\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879148db-ea13-408c-879a-ca873ac4a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot tables\n",
    "#Pivoting on one variable\n",
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values = \"weekly_sales\", index = \"type\")\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n",
    "\n",
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(values = \"weekly_sales\", index = \"type\", aggfunc = [np.mean, np.median])\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)\n",
    "\n",
    "# Pivot for mean weekly_sales by store type and holiday \n",
    "mean_sales_by_type_holiday = sales.pivot_table(values = \"weekly_sales\", index = \"type\", columns = \"is_holiday\")\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a87be9-3b09-42a7-ae4e-363cc5aa131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in missing values and sum values with pivot tables\n",
    "\n",
    "# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(values = \"weekly_sales\", index = \"department\", columns = \"type\", fill_value = 0))\n",
    "\n",
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value = 0 , margins = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b186a-6c79-4e8f-a17b-584c51d96f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explicit indexes\n",
    "#Setting and removing indexes\n",
    "# Look at temperatures\n",
    "print(temperatures)\n",
    "\n",
    "# Index temperatures by city\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "\n",
    "# Look at temperatures_ind\n",
    "print(temperatures_ind)\n",
    "\n",
    "# Reset the index, keeping its contents\n",
    "print(temperatures_ind.reset_index())\n",
    "\n",
    "# Reset the index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6c440-782c-4402-b866-6863a17e46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting with .loc[]\n",
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures[\"city\"].isin(cities)])\n",
    "\n",
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[cities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd064e-b799-4411-996c-1b0301585fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting multi-level indexes\n",
    "# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
    "\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n",
    "\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.loc[rows_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a4318-d6d3-40da-aa68-676510e1c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting by index values\n",
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index())\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level = \"city\"))\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level = [\"country\",\"city\"], ascending = [True, False]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62483ec1-fef6-422f-8ac1-80b03bbabe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slicing and subsetting with .loc and .iloc\n",
    "#Slicing index values\n",
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "\n",
    "# Subset rows from Pakistan to Russia\n",
    "print(temperatures_srt.loc[\"Pakistan\" : \"Russia\"])\n",
    "\n",
    "# Try to subset rows from Lahore to Moscow\n",
    "print(temperatures_srt.loc[\"Lahore\" : \"Moscow\"])\n",
    "\n",
    "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(temperatures_srt.loc[(\"Pakistan\", \"Lahore\") : (\"Russia\", \"Moscow\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9360d4a-0b34-4843-bd85-1d931820a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slicing in both directions\n",
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[(\"India\",\"Hyderabad\") : (\"Iraq\",\"Baghdad\")])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[: ,\"date\" : \"avg_temp_c\"])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[ (\"India\",\"Hyderabad\") : (\"Iraq\",\"Baghdad\"), \"date\" :\"avg_temp_c\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ee681-ebb8-4823-8ffd-2595b1dade70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slicing time series\n",
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-31\")]\n",
    "print(temperatures_bool)\n",
    "\n",
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index(\"date\").sort_index()\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(temperatures_ind.loc[\"2010\":\"2011\"])\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(temperatures_ind.loc[\"2010-08-01\":\"2011-02-28\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f04ddd-9c77-45ce-a9e9-8d2556db1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting by row/column number\n",
    "# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(temperatures.iloc[22,1])\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[0:5])\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:,2:4])\n",
    "\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[0:5, 2:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44428976-0910-469b-a6aa-a1c5e42cb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with pivot tables\n",
    "#Pivot temperature by city and year\n",
    "# Add a year column to temperatures\n",
    "temperatures[\"year\"] = temperatures[\"date\"].dt.year\n",
    "\n",
    "# Pivot avg_temp_c by country and city vs year\n",
    "temp_by_country_city_vs_year = temperatures.pivot_table(\"avg_temp_c\", index = [\"country\", \"city\"], columns = \"year\")\n",
    "\n",
    "# See the result\n",
    "print(temp_by_country_city_vs_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e925e5c-0cc1-4bfe-ae8b-b5795b56d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting pivot tables\n",
    "# Subset for Egypt to India\n",
    "temp_by_country_city_vs_year.loc[\"Egypt\":\"India\"]\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi\n",
    "temp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\"): (\"India\",\"Delhi\")]\n",
    "\n",
    "# Subset in both directions at once\n",
    "temp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\"): (\"India\",\"Delhi\"), \"2005\":\"2010\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b42c6-d315-4de6-a6b8-2483dbebf38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating on a pivot table\n",
    "# Get the worldwide mean temp by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "\n",
    "# Filter for the year that had the highest mean temp\n",
    "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
    "\n",
    "# Get the mean temp by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis = \"columns\")\n",
    "\n",
    "# Filter for the city that had the lowest mean temp\n",
    "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f36e9a-4b0d-4fac-88aa-e1f5ca4320b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing your data\n",
    "#Which avocado size is most popular?\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(avocados.head())\n",
    "\n",
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = avocados.groupby(\"size\")[\"nb_sold\"].sum()\n",
    "\n",
    "# Create a bar plot of the number of avocados sold by size\n",
    "nb_sold_by_size.plot(kind = \"bar\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995aba8e-1628-4b6a-856c-e671255a9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes in sales over time\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the total number of avocados sold on each date\n",
    "nb_sold_by_date = avocados.groupby(\"date\")[\"nb_sold\"].sum()\n",
    "\n",
    "# Create a line plot of the number of avocados sold by date\n",
    "nb_sold_by_date.plot(kind = \"line\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3f2f9-68cd-4a46-a59e-71ac9183d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avocado supply and demand\n",
    "# Scatter plot of nb_sold vs avg_price with title\n",
    "avocados.plot(x = \"nb_sold\", y= \"avg_price\", kind = \"scatter\", title = \"Number of avocados sold vs. average price\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece58ef-91c1-4d8d-b4f0-7200513ec478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Price of conventional vs. organic avocados\n",
    "# Histogram of conventional avg_price \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist()\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Modify histogram transparency to 0.5 \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha = 0.5)\n",
    "\n",
    "# Modify histogram transparency to 0.5\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha = 0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Modify bins to 20\n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\n",
    "\n",
    "# Modify bins to 20\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa58b3-a330-4fb5-a7a9-5245cf36fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values\n",
    "#Finding missing values\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind = \"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c55dbb-9d6a-4a4a-9590-3610df07cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing missing values\n",
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(avocados_complete.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51914e-e8e7-4618-bb5e-039b389f50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing missing values\n",
    "# List the columns with missing values\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "\n",
    "# Create histograms showing the distributions cols_with_missing\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# From previous step\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "plt.show()\n",
    "\n",
    "# Fill in missing values with 0\n",
    "avocados_filled = avocados_2016.fillna(0)\n",
    "\n",
    "# Create histograms of the filled columns\n",
    "avocados_filled[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69493b-4f2c-4466-bb49-dfe2228b5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DataFrames\n",
    "#List of dictionaries\n",
    "# Create a list of dictionaries with new data\n",
    "avocados_list = [\n",
    "    {\"date\": \"2019-11-03\", \"small_sold\": 10376832, \"large_sold\": 7835071},\n",
    "    {\"date\": \"2019-11-10\", \"small_sold\": 10717154, \"large_sold\": 8561348},\n",
    "]\n",
    "\n",
    "# Convert list into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_list)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e3822-aae1-4153-820c-45513efcc0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary of lists\n",
    "# Create a dictionary of lists with new data\n",
    "avocados_dict = {\n",
    "  \"date\": [\"2019-11-17\", \"2019-12-01\"],\n",
    "  \"small_sold\": [10859987, 9291631],\n",
    "  \"large_sold\": [7674135, 6238096]\n",
    "}\n",
    "\n",
    "# Convert dictionary into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_dict)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2081bda-3951-4c42-b9c9-1eb92bbe5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading and writing CSVs\n",
    "#CSV to DataFrame\n",
    "# Read CSV as DataFrame called airline_bumping\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "\n",
    "# Take a look at the DataFrame\n",
    "print(airline_bumping.head())\n",
    "\n",
    "# From previous step\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "print(airline_bumping.head())\n",
    "\n",
    "# For each airline, select nb_bumped and total_passengers and sum\n",
    "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
    "\n",
    "# From previous steps\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "print(airline_bumping.head())\n",
    "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
    "\n",
    "# Create new col, bumps_per_10k: no. of bumps per 10k passengers for each airline\n",
    "airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n",
    "\n",
    "# From previous steps\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "print(airline_bumping.head())\n",
    "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
    "airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n",
    "\n",
    "# Print airline_totals\n",
    "print(airline_totals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806388b-ab8a-4f19-b464-8c6b73391334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame to CSV\n",
    "# Create airline_totals_sorted\n",
    "airline_totals_sorted = airline_totals.sort_values(\"bumps_per_10k\", ascending = False)\n",
    "\n",
    "# Print airline_totals_sorted\n",
    "print(airline_totals_sorted)\n",
    "\n",
    "# Save as airline_totals_sorted.csv\n",
    "airline_totals_sorted.to_csv(\"airline_totals_sorted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d5360-1d7c-48a3-a206-4f5b82a9528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
